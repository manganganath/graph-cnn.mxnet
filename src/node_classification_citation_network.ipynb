{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon\n",
    "from utils import load_data, accuracy\n",
    "from model import GCN\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cited</th>\n",
       "      <th>citing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>103482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>103515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>1050679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>1103960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cited   citing\n",
       "0     35     1033\n",
       "1     35   103482\n",
       "2     35   103515\n",
       "3     35  1050679\n",
       "4     35  1103960"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_data = pd.read_csv('/home/ec2-user/SageMaker/graph-cnn.mxnet/data/cora/cora.cites', sep = '\\t', header=None)\n",
    "cite_data.columns = ['cited', 'citing']\n",
    "cite_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1425</th>\n",
       "      <th>1426</th>\n",
       "      <th>1427</th>\n",
       "      <th>1428</th>\n",
       "      <th>1429</th>\n",
       "      <th>1430</th>\n",
       "      <th>1431</th>\n",
       "      <th>1432</th>\n",
       "      <th>1433</th>\n",
       "      <th>1434</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31336</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1061127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rule_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1106406</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1126012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1107140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1102850</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1106418</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Theory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1435 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     \\\n",
       "0    31336     0     0     0     0     0     0     0     0     0   \n",
       "1  1061127     0     0     0     0     0     0     0     0     0   \n",
       "2  1106406     0     0     0     0     0     0     0     0     0   \n",
       "3    13195     0     0     0     0     0     0     0     0     0   \n",
       "4    37879     0     0     0     0     0     0     0     0     0   \n",
       "5  1126012     0     0     0     0     0     0     0     0     0   \n",
       "6  1107140     0     0     0     0     0     0     0     0     0   \n",
       "7  1102850     0     0     0     1     0     0     0     0     0   \n",
       "8    31349     0     0     0     0     0     0     0     0     0   \n",
       "9  1106418     0     0     0     0     0     0     0     0     0   \n",
       "\n",
       "            ...            1425  1426  1427  1428  1429  1430  1431  1432  \\\n",
       "0           ...               0     0     1     0     0     0     0     0   \n",
       "1           ...               0     1     0     0     0     0     0     0   \n",
       "2           ...               0     0     0     0     0     0     0     0   \n",
       "3           ...               0     0     0     0     0     0     0     0   \n",
       "4           ...               0     0     0     0     0     0     0     0   \n",
       "5           ...               0     0     1     0     0     0     0     0   \n",
       "6           ...               0     0     0     0     0     0     0     0   \n",
       "7           ...               0     0     0     0     0     0     0     0   \n",
       "8           ...               0     0     0     0     0     0     0     0   \n",
       "9           ...               0     0     0     0     0     0     0     0   \n",
       "\n",
       "   1433                    1434  \n",
       "0     0         Neural_Networks  \n",
       "1     0           Rule_Learning  \n",
       "2     0  Reinforcement_Learning  \n",
       "3     0  Reinforcement_Learning  \n",
       "4     0   Probabilistic_Methods  \n",
       "5     0   Probabilistic_Methods  \n",
       "6     0                  Theory  \n",
       "7     0         Neural_Networks  \n",
       "8     0         Neural_Networks  \n",
       "9     0                  Theory  \n",
       "\n",
       "[10 rows x 1435 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_data = pd.read_csv('/home/ec2-user/SageMaker/graph-cnn.mxnet/data/cora/cora.content', sep = '\\t', header=None)\n",
    "content_data.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Genetic_Algorithms': 0,\n",
       " 'Rule_Learning': 1,\n",
       " 'Theory': 2,\n",
       " 'Neural_Networks': 3,\n",
       " 'Probabilistic_Methods': 4,\n",
       " 'Reinforcement_Learning': 5,\n",
       " 'Case_Based': 6}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode class labels \n",
    "cls_labels = dict()\n",
    "for i, c in enumerate(set(content_data.iloc[:,1434].tolist())):\n",
    "    cls_labels[c] = i\n",
    "    \n",
    "cls_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and visualize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 2708\n",
      "Number of edges: 5429\n",
      "Average in degree:   2.0048\n",
      "Average out degree:   2.0048\n"
     ]
    }
   ],
   "source": [
    "G = nx.from_pandas_edgelist(cite_data, source='citing', target='cited',  create_using=nx.DiGraph())\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate node positions for visualization\n",
    "pos = nx.spring_layout(G, iterations=100)\n",
    "\n",
    "#### color coding nodes based on their classes\n",
    "node_color = [cls_labels[content_data.ix[content_data.ix[:,0]==node,1434].tolist()[0]] for node in G.nodes()]\n",
    "\n",
    "fig = plt.figure(figsize=(50,50))\n",
    "plt.axis(\"off\")\n",
    "plt.title('Citation Network', fontsize=50)\n",
    "nx.draw_networkx(G, pos = pos, cmap = plt.get_cmap(\"jet\"), node_color = node_color, with_labels = False, width = 0.8, linewidths=0.1)\n",
    "plt.show()\n",
    "fig.savefig('citation_net.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Citation Network](citation_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "seed = 42\n",
    "dropout = 0.0\n",
    "hidden = 16\n",
    "epochs = 200\n",
    "weight_decay = 5e-4\n",
    "optim = 'adam'\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss() # Original implementation uses Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seed for random number generators in numpy and mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "mx.random.seed(seed)\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(ctx=ctx)\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=int(labels.max().asnumpy().item()) + 1,\n",
    "            dropout=dropout)\n",
    "\n",
    "model.collect_params().initialize(ctx=ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': lr,})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 4.1402 acc_train: 0.1000 time: 0.0109s\n",
      "Epoch: 0002 loss_train: 3.5908 acc_train: 0.1000 time: 0.0115s\n",
      "Epoch: 0003 loss_train: 3.1512 acc_train: 0.1000 time: 0.0054s\n",
      "Epoch: 0004 loss_train: 2.8400 acc_train: 0.1000 time: 0.0042s\n",
      "Epoch: 0005 loss_train: 2.6206 acc_train: 0.2929 time: 0.0042s\n",
      "Epoch: 0006 loss_train: 2.4357 acc_train: 0.2929 time: 0.0050s\n",
      "Epoch: 0007 loss_train: 2.2610 acc_train: 0.2929 time: 0.0043s\n",
      "Epoch: 0008 loss_train: 2.1008 acc_train: 0.2929 time: 0.0049s\n",
      "Epoch: 0009 loss_train: 1.9719 acc_train: 0.2929 time: 0.0050s\n",
      "Epoch: 0010 loss_train: 1.8906 acc_train: 0.2929 time: 0.0053s\n",
      "Epoch: 0011 loss_train: 1.8608 acc_train: 0.2500 time: 0.0048s\n",
      "Epoch: 0012 loss_train: 1.8676 acc_train: 0.2000 time: 0.0050s\n",
      "Epoch: 0013 loss_train: 1.8869 acc_train: 0.2000 time: 0.0050s\n",
      "Epoch: 0014 loss_train: 1.9006 acc_train: 0.2000 time: 0.0043s\n",
      "Epoch: 0015 loss_train: 1.9010 acc_train: 0.2000 time: 0.0053s\n",
      "Epoch: 0016 loss_train: 1.8875 acc_train: 0.2071 time: 0.0051s\n",
      "Epoch: 0017 loss_train: 1.8637 acc_train: 0.3286 time: 0.0048s\n",
      "Epoch: 0018 loss_train: 1.8355 acc_train: 0.3500 time: 0.0052s\n",
      "Epoch: 0019 loss_train: 1.8086 acc_train: 0.4143 time: 0.0048s\n",
      "Epoch: 0020 loss_train: 1.7872 acc_train: 0.6071 time: 0.0046s\n",
      "Epoch: 0021 loss_train: 1.7725 acc_train: 0.3214 time: 0.0046s\n",
      "Epoch: 0022 loss_train: 1.7630 acc_train: 0.2929 time: 0.0050s\n",
      "Epoch: 0023 loss_train: 1.7565 acc_train: 0.2929 time: 0.0047s\n",
      "Epoch: 0024 loss_train: 1.7505 acc_train: 0.2929 time: 0.0046s\n",
      "Epoch: 0025 loss_train: 1.7426 acc_train: 0.2929 time: 0.0049s\n",
      "Epoch: 0026 loss_train: 1.7313 acc_train: 0.2929 time: 0.0047s\n",
      "Epoch: 0027 loss_train: 1.7158 acc_train: 0.2929 time: 0.0046s\n",
      "Epoch: 0028 loss_train: 1.6967 acc_train: 0.2929 time: 0.0048s\n",
      "Epoch: 0029 loss_train: 1.6750 acc_train: 0.2929 time: 0.0039s\n",
      "Epoch: 0030 loss_train: 1.6525 acc_train: 0.2929 time: 0.0039s\n",
      "Epoch: 0031 loss_train: 1.6310 acc_train: 0.3143 time: 0.0038s\n",
      "Epoch: 0032 loss_train: 1.6120 acc_train: 0.4143 time: 0.0039s\n",
      "Epoch: 0033 loss_train: 1.5966 acc_train: 0.4500 time: 0.0039s\n",
      "Epoch: 0034 loss_train: 1.5848 acc_train: 0.4786 time: 0.0038s\n",
      "Epoch: 0035 loss_train: 1.5760 acc_train: 0.4929 time: 0.0039s\n",
      "Epoch: 0036 loss_train: 1.5689 acc_train: 0.5071 time: 0.0039s\n",
      "Epoch: 0037 loss_train: 1.5623 acc_train: 0.5214 time: 0.0039s\n",
      "Epoch: 0038 loss_train: 1.5549 acc_train: 0.5357 time: 0.0039s\n",
      "Epoch: 0039 loss_train: 1.5462 acc_train: 0.5429 time: 0.0037s\n",
      "Epoch: 0040 loss_train: 1.5364 acc_train: 0.5429 time: 0.0038s\n",
      "Epoch: 0041 loss_train: 1.5258 acc_train: 0.5429 time: 0.0040s\n",
      "Epoch: 0042 loss_train: 1.5150 acc_train: 0.5429 time: 0.0041s\n",
      "Epoch: 0043 loss_train: 1.5045 acc_train: 0.5143 time: 0.0039s\n",
      "Epoch: 0044 loss_train: 1.4945 acc_train: 0.4929 time: 0.0039s\n",
      "Epoch: 0045 loss_train: 1.4850 acc_train: 0.4643 time: 0.0038s\n",
      "Epoch: 0046 loss_train: 1.4757 acc_train: 0.4500 time: 0.0039s\n",
      "Epoch: 0047 loss_train: 1.4665 acc_train: 0.4500 time: 0.0040s\n",
      "Epoch: 0048 loss_train: 1.4569 acc_train: 0.4429 time: 0.0039s\n",
      "Epoch: 0049 loss_train: 1.4470 acc_train: 0.4429 time: 0.0040s\n",
      "Epoch: 0050 loss_train: 1.4367 acc_train: 0.4643 time: 0.0038s\n",
      "Epoch: 0051 loss_train: 1.4260 acc_train: 0.4714 time: 0.0036s\n",
      "Epoch: 0052 loss_train: 1.4151 acc_train: 0.4929 time: 0.0038s\n",
      "Epoch: 0053 loss_train: 1.4041 acc_train: 0.5071 time: 0.0040s\n",
      "Epoch: 0054 loss_train: 1.3933 acc_train: 0.5214 time: 0.0038s\n",
      "Epoch: 0055 loss_train: 1.3826 acc_train: 0.5357 time: 0.0040s\n",
      "Epoch: 0056 loss_train: 1.3722 acc_train: 0.5571 time: 0.0038s\n",
      "Epoch: 0057 loss_train: 1.3619 acc_train: 0.5714 time: 0.0039s\n",
      "Epoch: 0058 loss_train: 1.3518 acc_train: 0.5714 time: 0.0040s\n",
      "Epoch: 0059 loss_train: 1.3416 acc_train: 0.5857 time: 0.0039s\n",
      "Epoch: 0060 loss_train: 1.3314 acc_train: 0.5929 time: 0.0040s\n",
      "Epoch: 0061 loss_train: 1.3211 acc_train: 0.5857 time: 0.0039s\n",
      "Epoch: 0062 loss_train: 1.3106 acc_train: 0.6000 time: 0.0041s\n",
      "Epoch: 0063 loss_train: 1.2999 acc_train: 0.6071 time: 0.0040s\n",
      "Epoch: 0064 loss_train: 1.2892 acc_train: 0.6071 time: 0.0040s\n",
      "Epoch: 0065 loss_train: 1.2783 acc_train: 0.6071 time: 0.0039s\n",
      "Epoch: 0066 loss_train: 1.2674 acc_train: 0.6143 time: 0.0039s\n",
      "Epoch: 0067 loss_train: 1.2564 acc_train: 0.6214 time: 0.0039s\n",
      "Epoch: 0068 loss_train: 1.2453 acc_train: 0.6357 time: 0.0041s\n",
      "Epoch: 0069 loss_train: 1.2342 acc_train: 0.6429 time: 0.0041s\n",
      "Epoch: 0070 loss_train: 1.2229 acc_train: 0.6500 time: 0.0038s\n",
      "Epoch: 0071 loss_train: 1.2116 acc_train: 0.6571 time: 0.0037s\n",
      "Epoch: 0072 loss_train: 1.2002 acc_train: 0.6857 time: 0.0039s\n",
      "Epoch: 0073 loss_train: 1.1888 acc_train: 0.7286 time: 0.0040s\n",
      "Epoch: 0074 loss_train: 1.1773 acc_train: 0.7286 time: 0.0039s\n",
      "Epoch: 0075 loss_train: 1.1658 acc_train: 0.7357 time: 0.0039s\n",
      "Epoch: 0076 loss_train: 1.1543 acc_train: 0.7357 time: 0.0035s\n",
      "Epoch: 0077 loss_train: 1.1428 acc_train: 0.7500 time: 0.0040s\n",
      "Epoch: 0078 loss_train: 1.1313 acc_train: 0.7500 time: 0.0038s\n",
      "Epoch: 0079 loss_train: 1.1197 acc_train: 0.7571 time: 0.0039s\n",
      "Epoch: 0080 loss_train: 1.1081 acc_train: 0.7643 time: 0.0039s\n",
      "Epoch: 0081 loss_train: 1.0965 acc_train: 0.7643 time: 0.0040s\n",
      "Epoch: 0082 loss_train: 1.0848 acc_train: 0.7643 time: 0.0039s\n",
      "Epoch: 0083 loss_train: 1.0730 acc_train: 0.7643 time: 0.0038s\n",
      "Epoch: 0084 loss_train: 1.0612 acc_train: 0.7643 time: 0.0038s\n",
      "Epoch: 0085 loss_train: 1.0494 acc_train: 0.7714 time: 0.0039s\n",
      "Epoch: 0086 loss_train: 1.0375 acc_train: 0.7714 time: 0.0038s\n",
      "Epoch: 0087 loss_train: 1.0257 acc_train: 0.7786 time: 0.0039s\n",
      "Epoch: 0088 loss_train: 1.0139 acc_train: 0.7929 time: 0.0039s\n",
      "Epoch: 0089 loss_train: 1.0021 acc_train: 0.7929 time: 0.0039s\n",
      "Epoch: 0090 loss_train: 0.9903 acc_train: 0.7929 time: 0.0038s\n",
      "Epoch: 0091 loss_train: 0.9785 acc_train: 0.8214 time: 0.0038s\n",
      "Epoch: 0092 loss_train: 0.9667 acc_train: 0.8214 time: 0.0041s\n",
      "Epoch: 0093 loss_train: 0.9550 acc_train: 0.8214 time: 0.0040s\n",
      "Epoch: 0094 loss_train: 0.9432 acc_train: 0.8286 time: 0.0038s\n",
      "Epoch: 0095 loss_train: 0.9315 acc_train: 0.8286 time: 0.0038s\n",
      "Epoch: 0096 loss_train: 0.9198 acc_train: 0.8357 time: 0.0039s\n",
      "Epoch: 0097 loss_train: 0.9081 acc_train: 0.8429 time: 0.0040s\n",
      "Epoch: 0098 loss_train: 0.8965 acc_train: 0.8429 time: 0.0038s\n",
      "Epoch: 0099 loss_train: 0.8849 acc_train: 0.8429 time: 0.0038s\n",
      "Epoch: 0100 loss_train: 0.8734 acc_train: 0.8571 time: 0.0040s\n",
      "Epoch: 0101 loss_train: 0.8619 acc_train: 0.8643 time: 0.0040s\n",
      "Epoch: 0102 loss_train: 0.8504 acc_train: 0.8643 time: 0.0039s\n",
      "Epoch: 0103 loss_train: 0.8391 acc_train: 0.8714 time: 0.0039s\n",
      "Epoch: 0104 loss_train: 0.8277 acc_train: 0.8786 time: 0.0039s\n",
      "Epoch: 0105 loss_train: 0.8164 acc_train: 0.8786 time: 0.0039s\n",
      "Epoch: 0106 loss_train: 0.8052 acc_train: 0.8786 time: 0.0040s\n",
      "Epoch: 0107 loss_train: 0.7941 acc_train: 0.8786 time: 0.0040s\n",
      "Epoch: 0108 loss_train: 0.7830 acc_train: 0.8857 time: 0.0038s\n",
      "Epoch: 0109 loss_train: 0.7721 acc_train: 0.8857 time: 0.0038s\n",
      "Epoch: 0110 loss_train: 0.7612 acc_train: 0.8929 time: 0.0039s\n",
      "Epoch: 0111 loss_train: 0.7503 acc_train: 0.8929 time: 0.0039s\n",
      "Epoch: 0112 loss_train: 0.7396 acc_train: 0.9000 time: 0.0040s\n",
      "Epoch: 0113 loss_train: 0.7289 acc_train: 0.9071 time: 0.0038s\n",
      "Epoch: 0114 loss_train: 0.7183 acc_train: 0.9143 time: 0.0037s\n",
      "Epoch: 0115 loss_train: 0.7079 acc_train: 0.9143 time: 0.0040s\n",
      "Epoch: 0116 loss_train: 0.6975 acc_train: 0.9143 time: 0.0040s\n",
      "Epoch: 0117 loss_train: 0.6871 acc_train: 0.9143 time: 0.0040s\n",
      "Epoch: 0118 loss_train: 0.6769 acc_train: 0.9143 time: 0.0038s\n",
      "Epoch: 0119 loss_train: 0.6668 acc_train: 0.9214 time: 0.0039s\n",
      "Epoch: 0120 loss_train: 0.6568 acc_train: 0.9214 time: 0.0041s\n",
      "Epoch: 0121 loss_train: 0.6469 acc_train: 0.9286 time: 0.0042s\n",
      "Epoch: 0122 loss_train: 0.6371 acc_train: 0.9357 time: 0.0039s\n",
      "Epoch: 0123 loss_train: 0.6274 acc_train: 0.9429 time: 0.0039s\n",
      "Epoch: 0124 loss_train: 0.6177 acc_train: 0.9429 time: 0.0038s\n",
      "Epoch: 0125 loss_train: 0.6082 acc_train: 0.9429 time: 0.0041s\n",
      "Epoch: 0126 loss_train: 0.5988 acc_train: 0.9429 time: 0.0039s\n",
      "Epoch: 0127 loss_train: 0.5896 acc_train: 0.9429 time: 0.0039s\n",
      "Epoch: 0128 loss_train: 0.5804 acc_train: 0.9429 time: 0.0039s\n",
      "Epoch: 0129 loss_train: 0.5713 acc_train: 0.9429 time: 0.0040s\n",
      "Epoch: 0130 loss_train: 0.5623 acc_train: 0.9429 time: 0.0039s\n",
      "Epoch: 0131 loss_train: 0.5535 acc_train: 0.9429 time: 0.0041s\n",
      "Epoch: 0132 loss_train: 0.5447 acc_train: 0.9429 time: 0.0039s\n",
      "Epoch: 0133 loss_train: 0.5361 acc_train: 0.9429 time: 0.0039s\n",
      "Epoch: 0134 loss_train: 0.5276 acc_train: 0.9429 time: 0.0038s\n",
      "Epoch: 0135 loss_train: 0.5192 acc_train: 0.9429 time: 0.0036s\n",
      "Epoch: 0136 loss_train: 0.5109 acc_train: 0.9429 time: 0.0039s\n",
      "Epoch: 0137 loss_train: 0.5027 acc_train: 0.9429 time: 0.0040s\n",
      "Epoch: 0138 loss_train: 0.4946 acc_train: 0.9571 time: 0.0039s\n",
      "Epoch: 0139 loss_train: 0.4867 acc_train: 0.9571 time: 0.0039s\n",
      "Epoch: 0140 loss_train: 0.4788 acc_train: 0.9571 time: 0.0040s\n",
      "Epoch: 0141 loss_train: 0.4711 acc_train: 0.9571 time: 0.0039s\n",
      "Epoch: 0142 loss_train: 0.4635 acc_train: 0.9571 time: 0.0038s\n",
      "Epoch: 0143 loss_train: 0.4560 acc_train: 0.9571 time: 0.0039s\n",
      "Epoch: 0144 loss_train: 0.4486 acc_train: 0.9571 time: 0.0040s\n",
      "Epoch: 0145 loss_train: 0.4414 acc_train: 0.9571 time: 0.0042s\n",
      "Epoch: 0146 loss_train: 0.4342 acc_train: 0.9571 time: 0.0039s\n",
      "Epoch: 0147 loss_train: 0.4272 acc_train: 0.9643 time: 0.0039s\n",
      "Epoch: 0148 loss_train: 0.4203 acc_train: 0.9643 time: 0.0038s\n",
      "Epoch: 0149 loss_train: 0.4134 acc_train: 0.9714 time: 0.0038s\n",
      "Epoch: 0150 loss_train: 0.4067 acc_train: 0.9714 time: 0.0039s\n",
      "Epoch: 0151 loss_train: 0.4001 acc_train: 0.9714 time: 0.0039s\n",
      "Epoch: 0152 loss_train: 0.3937 acc_train: 0.9714 time: 0.0039s\n",
      "Epoch: 0153 loss_train: 0.3873 acc_train: 0.9714 time: 0.0039s\n",
      "Epoch: 0154 loss_train: 0.3810 acc_train: 0.9714 time: 0.0039s\n",
      "Epoch: 0155 loss_train: 0.3749 acc_train: 0.9714 time: 0.0038s\n",
      "Epoch: 0156 loss_train: 0.3688 acc_train: 0.9714 time: 0.0043s\n",
      "Epoch: 0157 loss_train: 0.3628 acc_train: 0.9714 time: 0.0042s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0158 loss_train: 0.3570 acc_train: 0.9786 time: 0.0039s\n",
      "Epoch: 0159 loss_train: 0.3513 acc_train: 0.9786 time: 0.0035s\n",
      "Epoch: 0160 loss_train: 0.3456 acc_train: 0.9786 time: 0.0040s\n",
      "Epoch: 0161 loss_train: 0.3401 acc_train: 0.9786 time: 0.0039s\n",
      "Epoch: 0162 loss_train: 0.3346 acc_train: 0.9786 time: 0.0038s\n",
      "Epoch: 0163 loss_train: 0.3293 acc_train: 0.9786 time: 0.0039s\n",
      "Epoch: 0164 loss_train: 0.3240 acc_train: 0.9786 time: 0.0041s\n",
      "Epoch: 0165 loss_train: 0.3189 acc_train: 0.9786 time: 0.0040s\n",
      "Epoch: 0166 loss_train: 0.3138 acc_train: 0.9786 time: 0.0039s\n",
      "Epoch: 0167 loss_train: 0.3088 acc_train: 0.9786 time: 0.0039s\n",
      "Epoch: 0168 loss_train: 0.3039 acc_train: 0.9786 time: 0.0038s\n",
      "Epoch: 0169 loss_train: 0.2991 acc_train: 0.9786 time: 0.0039s\n",
      "Epoch: 0170 loss_train: 0.2944 acc_train: 0.9786 time: 0.0040s\n",
      "Epoch: 0171 loss_train: 0.2898 acc_train: 0.9786 time: 0.0037s\n",
      "Epoch: 0172 loss_train: 0.2853 acc_train: 0.9786 time: 0.0040s\n",
      "Epoch: 0173 loss_train: 0.2808 acc_train: 0.9786 time: 0.0039s\n",
      "Epoch: 0174 loss_train: 0.2765 acc_train: 0.9786 time: 0.0041s\n",
      "Epoch: 0175 loss_train: 0.2722 acc_train: 0.9786 time: 0.0040s\n",
      "Epoch: 0176 loss_train: 0.2680 acc_train: 0.9786 time: 0.0039s\n",
      "Epoch: 0177 loss_train: 0.2638 acc_train: 0.9786 time: 0.0037s\n",
      "Epoch: 0178 loss_train: 0.2598 acc_train: 0.9786 time: 0.0041s\n",
      "Epoch: 0179 loss_train: 0.2558 acc_train: 0.9786 time: 0.0038s\n",
      "Epoch: 0180 loss_train: 0.2519 acc_train: 0.9786 time: 0.0038s\n",
      "Epoch: 0181 loss_train: 0.2480 acc_train: 0.9786 time: 0.0039s\n",
      "Epoch: 0182 loss_train: 0.2443 acc_train: 0.9786 time: 0.0038s\n",
      "Epoch: 0183 loss_train: 0.2406 acc_train: 0.9786 time: 0.0039s\n",
      "Epoch: 0184 loss_train: 0.2370 acc_train: 0.9786 time: 0.0039s\n",
      "Epoch: 0185 loss_train: 0.2334 acc_train: 0.9857 time: 0.0037s\n",
      "Epoch: 0186 loss_train: 0.2299 acc_train: 0.9857 time: 0.0039s\n",
      "Epoch: 0187 loss_train: 0.2265 acc_train: 0.9857 time: 0.0040s\n",
      "Epoch: 0188 loss_train: 0.2231 acc_train: 0.9857 time: 0.0039s\n",
      "Epoch: 0189 loss_train: 0.2198 acc_train: 0.9857 time: 0.0038s\n",
      "Epoch: 0190 loss_train: 0.2165 acc_train: 0.9857 time: 0.0039s\n",
      "Epoch: 0191 loss_train: 0.2133 acc_train: 0.9929 time: 0.0039s\n",
      "Epoch: 0192 loss_train: 0.2102 acc_train: 0.9929 time: 0.0041s\n",
      "Epoch: 0193 loss_train: 0.2071 acc_train: 0.9929 time: 0.0039s\n",
      "Epoch: 0194 loss_train: 0.2041 acc_train: 0.9929 time: 0.0039s\n",
      "Epoch: 0195 loss_train: 0.2012 acc_train: 0.9929 time: 0.0039s\n",
      "Epoch: 0196 loss_train: 0.1982 acc_train: 0.9929 time: 0.0038s\n",
      "Epoch: 0197 loss_train: 0.1954 acc_train: 0.9929 time: 0.0041s\n",
      "Epoch: 0198 loss_train: 0.1926 acc_train: 0.9929 time: 0.0040s\n",
      "Epoch: 0199 loss_train: 0.1898 acc_train: 0.9929 time: 0.0038s\n",
      "Epoch: 0200 loss_train: 0.1871 acc_train: 0.9929 time: 0.0036s\n",
      "Training Accuracy:  0.9928571428571429 \n",
      " Validation Accuracy:  0.82 \n",
      " Test Accuracy:  0.808\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    with autograd.record():\n",
    "        output = model(features, adj)\n",
    "        loss_train = loss(output[idx_train], labels[idx_train])\n",
    "        acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "        accs.append(acc_train)\n",
    "        loss_train.backward()\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(np.mean(loss_train.asnumpy())),\n",
    "          'acc_train: {:.4f}'.format(acc_train),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    trainer.step(1)\n",
    "\n",
    "print(\n",
    "    'Training Accuracy: ', accuracy(output[idx_train], labels[idx_train]), '\\n',\n",
    "    'Validation Accuracy: ', accuracy(output[idx_val], labels[idx_val]), '\\n',\n",
    "    'Test Accuracy: ', accuracy(output[idx_test], labels[idx_test])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
